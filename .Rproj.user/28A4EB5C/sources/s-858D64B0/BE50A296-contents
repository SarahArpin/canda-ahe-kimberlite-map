---
title: "STAT 5000 Exam 2 Cheatsheet"
output:
  html_document: default
  pdf_document: default
---

## Unit 3: Expectation, Varience, Covarience 

<br>

##### Expected value of X

|    Discrete: $E(X) = \sum_x xP(X=x)$

|    Continuous: $E(X) = \int_{-\infty}^{\infty} x f(x) dx$

<br>

##### Expected value of $g(x)$

|    Discrete: $E(g(x)) = \sum_x g(x) P(X=x)$ 
*$P(X=x)$ can be the pmf of x itself*
  
|    Continuous: $E(g(x)) = \int_{-\infty}^{\infty} g(x) f(x) dx$  


<span style="color:#164589">**Theorm**: $E(aX + b) = aE(x) + b$</span>

<br>

##### Variance of X

|    Discrete: $Var(X) = \sigma^2 = \sum_x (x-\mu)^2 P(X=x)$ 
|              = $Var(X) = E[(x-\mu)^2]$ 

|    Continuous: $Var(X) = E[(x-\mu)^2]$ 


<span style="color:#164589">**Shortcut formula for variance**: $Var(X) = E(X^2) - [E(X)]^2$</span>

<br>

##### Varience of $g(x)$

|    Discrete: $Var(g(x)) = \sum (g(x)- \mu_g)^2 P(X = x)$

|    Continuous: $Var(g(x)) = \int_{-\infty}^{\infty} (g(x) - \mu_g)^2 f(x)dx$


<span style="color:#164589">**Shortcut formula if $g(x)$ is linear (i.e aX+b)**: $Var(g(x)) = Var(aX_b) = a^2 Var(X)$</span>
|    $\mu = E(X), \mu_g = E(aX + b) =a\mu + b$


*In R `var() = $S_x^2$` (ie. the sample variance). This is NOT = to $\sigma^2$* 

<br>

##### Standard deviation of X 

$sd(X) = \sigma = \sqrt{var(X)}$

<br>

##### Populations vs. Sample Statistics 

| Statistic | Sample | Population | 
|:------:|:------:| :------:|
| Average | $\bar{x}$ |  $\mu$ |
| Variance | $s^2$ | $\sigma^2$ |
| Standard Deviation| $s$ | $\sigma$ |
| Covariance | | | $Cov(X,Y)$ |
| Correlation | | $\rho_{x,y}$ |

<br>

##### Means and Variance of Distributions: 

| Distribution | Expected Value | Variance | 
|:------:|:------:| :------:|
| Geom(p) | $\frac{1}{p}$ |  $\frac{1-p}{p}$ |
| Poisson($\lambda)$ | $\lambda$ | $\lambda$ |
| Uniform(a,b) | $\frac{1}{2}(a+b)$ | $\frac{1}{12}(b-a)^2$ |
| Exp($\lambda$) | $\frac{1}{\lambda}$ | $\frac{1}{\lambda}^2$ |
| Beta($\alpha$,$\beta$) | $\frac{\alpha}{\alpha+ \beta}$ | $\frac{\alpha\beta}{[(\alpha + \beta)^2(\alpha+\beta+1)]}$ |

<br> 

##### Covariance of X

|    Discrete: $Cov(X,Y) = \sum_x \sum_y (x-\mu_x) (y-\mu_y) P(X=x, Y=y)$ 

*$P(X=x, Y=y)$ is the joint probability distribution = P(intersection)*

|    Continuous: $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (x-\mu_x) (y-\mu_y) f(x,y) dx dy$

* If the variable tend to deviate in the same direction (ie. both above the mean at the same time) then covariance will be positive and vice versa. 
* If variables aren't strongly related they will be near 0. 


<span style="color:#164589">**Shortcut for $Cov(X,Y)$**: $Cov(X,Y) = E(XY) - E(X)E(Y)$</span>

<br>

##### Correlation of X

$\rho_{x,y} = \frac{Cov(X,Y)}{\sigma(x)\sigma(Y)}$

*  Correlation is unitless and ranges between -1 and 1. 
* $\rho_{x,y} = 1 or -1$ is perfect correlation; $\rho_{x,y} = 0$ is no correlation. 
* If X,Y are independent, then they are uncorrelated $(Cov(X,Y) = 0 = \rho))$, but uncorrelated does **not** imply independence! 
* $\rho_{x,y}$ is a measure of the linear relationship between X and Y. two variables can be uncorrelated and highly dependent if their relationship is non-linear. 

<br> 

#### Properties:

Let $X_1...X_n$ be random variables and let $a_1...a_n$ be constants. Then:

1. $E(\sum_{i=1}^{n} a_i X_i) =  \sum_{i=1}^{n} a_i E(X_i)$
2. $Var(a_1X_1 \pm a_2X_2) = a_1^2 Var(X_1) + a_2^2Var(x_2) \pm 2a_1a_2Cov(X_1X_2)$
    i) If $X_1, X_2$ are independent, $Cov$ term $=0$
3. $Cov(a_1X_1 + a_3, a_2X_2 + a4) = a_1a_2 Cov(X_1,X_2)$ *Covariance between linear functions*
4. $Corr(a_1X_1 + a_3, a_2X_2 + a_4) = sgn(a_1,a_2)Corr(X_1,X_2)$
5. For any two random variables X and Y: $-1 \le Corr(X,Y) \le 1$

<br>

## Unit 4: The Normal Distribution and The Central Limit Theorm

<br> 

#### Normal Distribution 
$E(X) = \mu$

$Var(X) = \sigma^2$

$X \sim N(\mu, \sigma^2)$ 

PDF: $f(X) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}(x-\mu)^2}$

<br>

#### Standard Normal Distribution 
$\mu = 0$

$\sigma^2 = 1$

$Z \sim N(),1)$

PDF: $f(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$

<br>

Critical value: $Z_{\alpha}$ will denote the z-value for which $\alpha$ area under the standard normal curve lies to the right of $z_{\alpha}$

Standardizing variable: $Z = \frac{Z-\mu}{\sigma} \sim N(0,1)$

<br>

#### Statistical Models and Estimators

The random variables $X_1, X_2...X_n$ are said to form a simple random sample of size n if: 

1. $X_i$ is independent from $X_j, i \neq j$
2. $X_1....X_n$ are identically distributed. 


An estimator is a statistic put to the purpose of pinpointing or guessing a population parameter: 

$\bar{X} = \frac{1}{n} \sum X_i$ is an estimator of $\mu$

The sampling distribution of the sample mean depends on: 
1. The sample size, $n$
2. The population distribution 
3. Method of sampling
*The standard deviation of this distribution is called the the standard error of the estimator* 

Let $X_1, X_2,...X_n$ be a random sample from distribution with $E(X) = \mu$ and $Var(X) = \sigma^2$. Then: 

* $E(\bar{X}) = \mu$
* $Var(\bar{X}) = \frac{\sigma^2}{n}$
* $sd(\bar{X}) = \frac{\sigma}{\sqrt{n}}$ *Standard error of the mean*
* Distribution of sample mean: $\bar{X} = \frac{1}{n} \sum X_i \sim N(\mu, \frac{\sigma}{n})$

<br>

#### The Central Limit Theorm 

When the population distribution is non-normal, averaging produces a distribution more bell-shaped than the one being sampled. 

Let $\textbf{X} = (X_1,...X_n)$ be a set of iid random variables $E(X_i) = \mu \le \infty$ and $Var(X_i) = \sigma^2 \le \infty$ for all $i = 1...n$. Then: 

$\bar{X} \sim N(\mu, \frac{\sigma^2}{n})$

*The larger the value of n the better. Typically n > 30* 

<br> 

## Unit 5: Point and Interval Estimation 

<br>

#### Properties of 'good' estimators: 

* An estimator is **consistent** if, as n approaches $\infty$,  $P(|\hat{\theta}_n - \theta| \leq \epsilon) = 1$

*As the sample size gets bigger, the estimator converges to the true value* 

* The **bias** of an estimator $\hat{\theta}$ of parameter $\theta$, is defined as $B(\hat{\theta}) = E(\hat{\theta}) - \theta$
An estimator is unbiased if $E(\hat{\theta}) = \theta$

* An estimator of $\hat{\theta}$ of $\theta$, should have a small variance. 
The bias-variance trade off states that we can reduce the variance of an estimator by increasing the bias or vice versa) for a fixed population size. 

<br> 

Let $X_1...X_n \stackrel{iid}{\sim} N(\mu, \sigma^2)$:

1. $\bar{X} = \frac{1}{n} \sum X_i$ is a good estimator of $\mu$: 
  i) $\bar{X}$ is consistent 
  ii) $\bar{X}$ is unbiased
  iii) $\bar{X}$ has variance that gets smaller as sample size gets larger. $Var(\bar{X}) = \frac{\sigma^2}{n}$
  
2. $s^2 = \frac{1}{n-1} \sum (x_i - \bar{x})^2$ is a good estimator of $\sigma^2$
  i) $s^2$ is consistent 
  ii) $s^2$ is unbiased
  iii) $s^2$ has variance that gets smaller as sample size gets larger. 
  
<br>

#### Maximum Likelihood Estimation (MLE)

*The MLE of $\theta$ is an estimator of $\hat{\theta}$ that renders the data most likely. The MEL of $\theta$ is the value of $\theta$ that maximizes the likelihood function.* 

A likelihood function, denoted $L(\theta)$, is defined as any function proportion to the join pdf $F(x; \theta)$, but thought of as a function of $\theta$. 
|   - When data are independent: $f(x;\theta) = \prod_{i = 1}^{n} f(x_i; \theta)$

A log likelihood function, denoted by $\ell(\theta)$ is the $log$ of $L(\theta)$ 

The procedure: 

1. Find the pdf/pmf 
2. Find the joint pdf/pmf by taking the product of the pdf. Data is fixed, $\theta$ is variable. 
3. Take the log of the likelihood function (ie. the joint pdf written as a function of $\theta$)
4. Take the derivative, set it equal to 0, and solve for $\theta$. When you solve for $\theta$ it is equal to $\hat{\theta}$

<br>

#### Properties of the MLE: $\hat{\theta}_n$

1. **Consistency**. As n approaches $\infty, P(|\hat{\theta}_n - \theta| \leq \epsilon) = 1$
2. **Asymptotically unbiased**. As n approaches $\infty, E(\hat{\theta}_n) = 0$
3. **Efficient**. As n approaches $\infty, Var(\hat{\theta})$ is as small as possible. 
4. **Asymptotically normality**. $\hat{\theta}_n \sim N(\theta, \sigma_{\hat{\theta}_n}^2)$

<br>

#### Invariance property of MLE 

Let $\hat{\theta}_n$ be the MLE of $\theta$. Let $\tau(\theta)$ be some function of $\theta$. Then the MLE of $\tau(\theta)$ is $\tau(\hat{\theta}_n)$

*MLE's are for point estimates. They do not quantify uncertainty.* 

<br>

#### Interval Estimate | Confidence Intervals 

An *interval estimator* is a range of values used to estimate $\theta$. Quantifies uncertainty. 

<br>

#### The Z-distribution 

$Z = \frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}} \sim N(0,1)$

Random sample from normal population with know $\sigma$. Then a $(1-\alpha) \cdot 100%$ Z-confidence interval for the mean $\mu$: 

$$\bar{X} \pm Z_{\alpha/2}\frac{\sigma}{\sqrt{n}}$$

Example calculating alpha: 

* Confidence Interval = 95% 
* alpha = 1 - CI = 0.05 
* alpha/2 = 0.025 
* qnorm(1-0.025 OR 0.975)

<br>

#### When to use what distribution: 

|  | $n \ge 30$ | $n < 30$ | 
|:------:|:------:| :------:|
| Underlying normal | $\sigma$ known; $Z$ |  $\sigma$ known, $Z$ |
| Underlying normal | $\sigma$ unknown; $t$ or $Z$ | $\sigma$ unknown, $t$ |
| Underlying non-normal | $\sigma$ known; $Z$ | $\sigma$ known |
| Underlying non-normal | $\sigma$ unknown; $Z$ |$\sigma$ unknown |

<br>

When we don't know $\sigma$ (population sd), we need to work with the sample standard deviation:

$s^2 = \frac{1}{1-n} \sum(X_i - \bar{X]})^2$

$s = \sqrt{s^2}$
 
<br>

#### The t-distribution 

$T = S(X) = \frac{\bar{X} - \mu}{s/\sqrt{n}} \sim t_{n-1}$

Random sample from normal population with mean $\mu$, let $\bar{X}$ and $s^2$ be sample mean and sample variance. Then a $(1-\alpha) \cdot 100%$ t-confidence interval for $\mu$: 

$$\bar{X} \pm t_{\alpha/2, n-1} \frac{s}{\sqrt{n}}$$

Calculating critical value: qt(1-$\alpha$, $v$)

<br>

##### Properties of the t-distribution 

Let $t_v$ be the t-distribution with $v$ degrees of freedom $(n-1)$:
1. Bell-shaped and centered at 0 
2. More spread out than the z-curve
3. As $v$ increases, the spread decreases
4. Approaches normal as $v$ approaches $\infty$ 


If your underlying distribution is not normal, and $n < 30$, you either have to make a specific assumption about the form of the population distribution and derive a confidence interval based on that assumption, or use other methods (eg. bootstrapping), to make reasonable confidence intervals. 















